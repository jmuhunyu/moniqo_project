{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0286b2",
   "metadata": {},
   "source": [
    "Most important Hypothesis for me in this project is:\n",
    "\n",
    "1. Hypothesis 03. The more client has ability to pay off(istead of making default micro finance), they will choose 5 weeks credit ease product.\n",
    "-> 10 weeks credit ease clients make highest miss-instalments rate, 7 weeks credit ease clients make highest default rate.\n",
    "-> 7, 10 weeks credit ease clients borrow more amount of money, less payback on time so it's risk for company.\n",
    "# how about set up feature transpormation [ Weight of Evidence (WoE) Encoding ] ?\n",
    "\n",
    "2. Hypothesis 05. Risk of default micro finance is different by branch as their lifestlye, quality of life, average salary might be different.\n",
    "-> Engineer branch has higher rate to make defaulty. Depends on geographic, feature of city, risk of default rate can be impacted.\n",
    "# By using [ Branch × BusinessType Target Encoding ] we could predict high risk client ?\n",
    "\n",
    "3. Hypothesis 06. Frequent client has high risk to make default micro finance.\n",
    "-> Client those who use product 3-4 times has highest risk to make defaulty while 1-2, 5-6 times clients have lower risk.\n",
    "# How about set up [ Loan Cycle Binning ] so that AI can give high risk point according to cycle ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e00222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1565 entries, 0 to 3469\n",
      "Data columns (total 22 columns):\n",
      " #   Column                        Non-Null Count  Dtype         \n",
      "---  ------                        --------------  -----         \n",
      " 0   LoanId                        1565 non-null   int64         \n",
      " 1   AmountDisbursed               1565 non-null   int64         \n",
      " 2   Interest                      1565 non-null   int64         \n",
      " 3   LoanBalance                   1565 non-null   float64       \n",
      " 4   BorrowDate                    1565 non-null   datetime64[ns]\n",
      " 5   Product                       1565 non-null   object        \n",
      " 6   Branch                        1565 non-null   object        \n",
      " 7   ClearDate                     1565 non-null   datetime64[ns]\n",
      " 8   CustomerId                    1565 non-null   int64         \n",
      " 9   Gender                        1565 non-null   object        \n",
      " 10  LoanLimit                     1565 non-null   float64       \n",
      " 11  CreditScore                   1565 non-null   int64         \n",
      " 12  loan_repayment_txn_count      1565 non-null   int64         \n",
      " 13  loan_repayment_total_paid     1565 non-null   int64         \n",
      " 14  loan_repayment_first_payment  1565 non-null   datetime64[ns]\n",
      " 15  loan_repayment_last_payment   1565 non-null   datetime64[ns]\n",
      " 16  loan_overpayment              1565 non-null   int64         \n",
      " 17  MissedInstalments             1565 non-null   float64       \n",
      " 18  Industry                      1565 non-null   object        \n",
      " 19  BusinessType                  1565 non-null   object        \n",
      " 20  YearEstablished               1565 non-null   int64         \n",
      " 21  days_to_first                 1565 non-null   int64         \n",
      "dtypes: datetime64[ns](4), float64(3), int64(10), object(5)\n",
      "memory usage: 281.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1565, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# Load the main dataset \n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset \n",
    "df.head()\n",
    "\n",
    "# BorrowDate:  MM/DD/YYYY (Keep dayfirst=False)\n",
    "df[\"BorrowDate\"] = pd.to_datetime(df[\"BorrowDate\"], errors=\"coerce\", dayfirst=False)\n",
    "\n",
    "# problematic columns: Use format='mixed' to handle the mix of DD/MM/YYYY and YYYY-MM-DD\n",
    "df[\"loan_repayment_first_payment\"] = pd.to_datetime(df[\"loan_repayment_first_payment\"], errors=\"coerce\", dayfirst=True, format='mixed')\n",
    "df[\"loan_repayment_last_payment\"] = pd.to_datetime(df[\"loan_repayment_last_payment\"], errors=\"coerce\", dayfirst=True, format='mixed')\n",
    "\n",
    "# ClearDate:  YYYY-MM-DD\n",
    "df[\"ClearDate\"] = pd.to_datetime(df[\"ClearDate\"], errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "\n",
    "# Check the result\n",
    "df.head()\n",
    "\n",
    "# More data cleaning \n",
    "# A customer cannot make a repayment 100 days BEFORE they borrowed the money.\n",
    "# We allow a small buffer (-10 days) for potential booking errors, but anything less is garbage.\n",
    "df['days_to_first'] = (df['loan_repayment_first_payment'] - df['BorrowDate']).dt.days\n",
    "df = df[(df['days_to_first'] >= -10) | (df['days_to_first'].isna())].copy()\n",
    "\n",
    "# The data logs the same loan twice. We keep only the first instance.\n",
    "df = df.drop_duplicates(subset=['CustomerId', 'BorrowDate'], keep='first')\n",
    "\n",
    "# A loan cannot be cleared BEFORE it was borrowed.\n",
    "df = df[df['ClearDate'] >= df['BorrowDate']].copy()\n",
    "\n",
    "# A business established in 2024 cannot borrow money in 2023.\n",
    "df = df[df['YearEstablished'] <= df['BorrowDate'].dt.year].copy()\n",
    "\n",
    "# Cleanup to remove Product types (sme - 3 months, sme - 4 months and 12 weeks credit ease) fron the dataset\n",
    "df = df[~df['Product'].isin(['sme - 3 months', 'sme - 4 months', '12 weeks credit ease'])]\n",
    "df.info()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b6353",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "1. Feature Selection \n",
    "2. Encoding Categorical data\n",
    "3. Handling Missing values\n",
    "4. Feature Transformation\n",
    "5. Balancing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb6ec2",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Our dataset is already clean, so only drop Identifiers (LoanId, CustomerId)\n",
    "\n",
    "1. Remove Unique Identifiers: Drop LoanId and CustomerId - no predictive power in actual ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove LoanId and CustomerId from the datasets\n",
    "columns_to_drop = ['LoanId', 'CustomerId']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe057e",
   "metadata": {},
   "source": [
    "# Encoding Categorical data\n",
    "\n",
    "1. Gender & Branch: Use Binary Encoding (0 or 1) - as they only have two categories.\n",
    "\n",
    "2. Product type :  Use One-Hot Encoding. Since there are only 3 products, this won't create too many columns.\n",
    "\n",
    "3. Industry & BusinessType: Use Frequency Encoding - replacing the name with its percentage in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Gender & Branch: Use Binary Encoding (0 or 1) - as they only have two categories.\n",
    "## Binary Encoding for Gener / Branch\n",
    "df['Gender'] = df['Gender'].map({'male': 1, 'female': 0})\n",
    "df['Branch'] = df['Branch'].map({'engineer': 1, 'molo': 0})\n",
    "\n",
    "#2. Product type :  Use One-Hot Encoding. Since there are only 3 products, this won't create too many columns.\n",
    "## prefix='Product' adds a label to the new columns for better organization\n",
    "product_dummies = pd.get_dummies(df['Product'], prefix='Product')\n",
    "df = pd.concat([df, product_dummies], axis=1) # Join the new columns back to the main dataframe\n",
    "df = df.drop(columns=['Product']) # Drop the original 'Product' column (since it's now encoded)\n",
    "\n",
    "#3. Industry & BusinessType: Use Frequency Encoding - replacing the name with its percentage in the dataset.\n",
    "## This replaces the name with the percentage (0.0 to 1.0) of its occurrence\n",
    "industry_freq = df['Industry'].value_counts(normalize=True)\n",
    "df['Industry'] = df['Industry'].map(industry_freq)\n",
    "\n",
    "## Frequency Encoding for BusinessType\n",
    "business_type_freq = df['BusinessType'].value_counts(normalize=True)\n",
    "df['BusinessType'] = df['BusinessType'].map(business_type_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761295b7",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "Dataset is already clean and don't have missing values.\n",
    "\n",
    "1. Convert dates to avoid the error. - James already did in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a047b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_to_first'] = (df['loan_repayment_first_payment'] - df['BorrowDate']).dt.days\n",
    "df = df[(df['days_to_first'] >= -10) | (df['days_to_first'].isna())].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840ad1e",
   "metadata": {},
   "source": [
    "# Feature Transformation\n",
    "\n",
    "1. Log Transformation for AmountDisbursed, CreditScore, Loanlimit as those are highly skewed\n",
    "\n",
    "2. Loan Cycle Binning - According to EDA (Loan Frequency, clients who are in cycle 3-4 have high risk to make feault, while others have lower risk to make defaulty)\n",
    "\n",
    "3. Scaling (Standardization) - To ensure that a large number like LoanLimit (e.g., 10,000) doesn't overpower a small number like BusinessAge (e.g., 5), we scale all numerical inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58411c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Log Transformation for AmountDisbursed, CreditScore, Loanlimit as those are highly skewed\n",
    "# Log transform highly skewed variables\n",
    "skewed_cols = ['AmountDisbursed', 'CreditScore', 'LoanLimit', 'Interest']\n",
    "for col in skewed_cols:\n",
    "    df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "    \n",
    "\n",
    "# 2. Loan Cycle Binning - According to EDA (Loan Frequency, clients who are in cycle 3-4 have high risk to make feault, while others have lower risk to make defaulty)\n",
    "# A. Loan Cycle Binning (Your finding: 3-4 is high risk, 5-6 is lower)\n",
    "def bin_loan_cycle(count):\n",
    "    if count <= 2: return 'Low_Freq'\n",
    "    if 3 <= count <= 4: return 'High_Risk_Cycle'\n",
    "    if 5 <= count <= 6: return 'Stable_Cycle'\n",
    "    return 'Veteran'\n",
    "\n",
    "# Assuming 'LoanCount' is your column for number of loans per client\n",
    "df['Cycle_Group'] = df['loan_repayment_txn_count'].map(bin_loan_cycle) # or appropriate column\n",
    "\n",
    "# B. Transaction Count Binning (Your finding: 0-4 highest, 50+ increasing)\n",
    "df['Txn_Group'] = pd.cut(df['loan_repayment_txn_count'], \n",
    "                         bins=[-1, 4, 49, 1000], \n",
    "                         labels=['Highest_Risk_Txn', 'Mild_Risk_Txn', 'Increased_Risk_Txn'])\n",
    "\n",
    "# C. Missed Installment Binning (Your finding: 0 is high, 1-5 lower, 6 highest)\n",
    "def bin_missed(count):\n",
    "    if count == 0: return 'No_Miss_Unexpected_Risk'\n",
    "    if 1 <= count <= 5: return 'Typical_Delay'\n",
    "    return 'Critical_Default'\n",
    "\n",
    "df['Missed_Group'] = df['MissedInstalments'].map(bin_missed)\n",
    "\n",
    "\n",
    "\n",
    "#3. Scaling (Standardization) - To ensure that a large number like LoanLimit (e.g., 10,000) doesn't overpower a small number like BusinessAge (e.g., 5), we scale all numerical inputs.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_features = ['BusinessAge', 'log_AmountDisbursed', 'log_CreditScore', 'Industry_Freq']\n",
    "df[num_features] = scaler.fit_transform(df[num_features])\n",
    "\n",
    "\n",
    "\n",
    "# 4. Interaction Features - Combining Product type with Credit Score bins to capture nuanced risk profiles.\n",
    "# Discretize CreditScore into 5 bins\n",
    "df['CreditScore_Bin'] = pd.qcut(df['CreditScore'], q=5, labels=False)\n",
    "\n",
    "# Create interaction feature between Product type and Credit Score bin\n",
    "df['Prod_Credit_Combo'] = df['Product'].astype(str) + \"_\" + df['CreditScore_Bin'].astype(str)\n",
    "\n",
    "# Calculate mean default rate for each combination\n",
    "mean_default = df.groupby('Prod_Credit_Combo')['is_default'].transform('mean')\n",
    "df['Prod_Credit_DefaultRate'] = mean_default\n",
    "\n",
    "\n",
    "# 5. Branch × BusinessType Target Encoding\n",
    "# Create a combined feature of Branch and BusinessType\n",
    "df['Branch_Biz_Combo'] = df['Branch'].astype(str) + \"_\" + df['BusinessType'].astype(str)\n",
    "\n",
    "# Calculate the mean default rate for each combination\n",
    "# and map it back to the dataframe\n",
    "branch_biz_risk = df.groupby('Branch_Biz_Combo')['is_default'].transform('mean')\n",
    "df['Branch_Biz_Risk_Score'] = branch_biz_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f9213",
   "metadata": {},
   "source": [
    "# Balancing datasets -> Using SMOTE to balance the data when we train the AI\n",
    "\n",
    "The initial 50/50 split was based on anyone missing an installment. However, our refined business definition of default—unpaid balance past the clear date—shows that only 8.5% of our clients are in actual default. Because this is an imbalanced dataset, we need to use SMOTE or balanced class weights to ensure the model doesn't overlook the minority group.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
