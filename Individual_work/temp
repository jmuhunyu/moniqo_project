# =========================
# HistGradientBoosting + your preprocess (dense output) + VAL threshold tuning + TEST
# =========================

import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve

# -------------------------
# 0) Build/ensure preprocess outputs DENSE (required by HistGradientBoosting)
# -------------------------
cat_cols = X_train.select_dtypes(include=["object", "category"]).columns
num_cols = X_train.columns.difference(cat_cols)

# sklearn >= 1.2 uses sparse_output; fallback to sparse for older versions
try:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
except TypeError:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse=False)

preprocess = ColumnTransformer(
    transformers=[
        ("cat", ohe, cat_cols),
        ("num", "passthrough", num_cols),
    ],
    remainder="drop",
    sparse_threshold=0.0,  # force dense even if something tries to be sparse
)

# -------------------------
# 1) Define pipeline model
# -------------------------
gb_model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", HistGradientBoostingClassifier(
        random_state=42,
        max_iter=800,
        learning_rate=0.03,
        max_depth=None,
        min_samples_leaf=30,
        l2_regularization=1.0
    ))
])

# -------------------------
# 2) Fit (optionally with sample_weight for imbalance)
# -------------------------
# OPTIONAL imbalance handling (recommended if class 1 is rare)
pos_weight = (y_train == 0).sum() / max((y_train == 1).sum(), 1)
sample_weight = np.where(y_train == 1, pos_weight, 1.0)

gb_model.fit(X_train, y_train, clf__sample_weight=sample_weight)

print("✅ Step 1 complete: Model trained on TRAIN set")

# -------------------------
# 3) Validation: ROC-AUC + threshold selection
# -------------------------
val_proba_gb = gb_model.predict_proba(X_val)[:, 1]
print("\nGB VAL ROC–AUC:", roc_auc_score(y_val, val_proba_gb))

precision, recall, thresholds = precision_recall_curve(y_val, val_proba_gb)

pr_df_gb = pd.DataFrame({
    "threshold": thresholds,
    "precision": precision[:-1],
    "recall": recall[:-1],
})

cand = (
    pr_df_gb[(pr_df_gb["recall"] >= 0.65) & (pr_df_gb["recall"] <= 0.80)]
    .sort_values("precision", ascending=False)
)

print("\nTop threshold candidates (VAL):")
display(cand.head(10))

CHOSEN_THRESHOLD_GB = float(cand.iloc[0]["threshold"]) if len(cand) else 0.5
print("\nChosen GB threshold:", CHOSEN_THRESHOLD_GB)

val_pred_gb = (val_proba_gb >= CHOSEN_THRESHOLD_GB).astype(int)
print("\nGB VAL classification report (chosen threshold):")
print(classification_report(y_val, val_pred_gb))

# -------------------------
# 4) Test: ROC-AUC + classification report using SAME threshold
# -------------------------
test_proba_gb = gb_model.predict_proba(X_test)[:, 1]
print("\nGB TEST ROC–AUC:", roc_auc_score(y_test, test_proba_gb))

test_pred_gb = (test_proba_gb >= CHOSEN_THRESHOLD_GB).astype(int)
print("\nGB TEST classification report (chosen threshold from VAL):")
print(classification_report(y_test, test_pred_gb))


# ========================= Old random forest code below =========================
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.metrics import precision_recall_curve
import pandas as pd

rf = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", RandomForestClassifier(
        n_estimators=600,
        max_depth=None,
        min_samples_leaf=10,     
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    ))
])

rf.fit(X_train, y_train)

# --- VAL ---
val_proba = rf.predict_proba(X_val)[:, 1]
print("RF VAL ROC–AUC:", roc_auc_score(y_val, val_proba))

precision, recall, thresholds = precision_recall_curve(y_val, val_proba)
pr_df = pd.DataFrame({
    "threshold": thresholds,
    "precision": precision[:-1],
    "recall": recall[:-1],
})

candidates = (pr_df[(pr_df["recall"] >= 0.65) & (pr_df["recall"] <= 0.80)]
              .sort_values("precision", ascending=False))

display(candidates.head(10))

CHOSEN_THRESHOLD_RF = float(candidates.iloc[0]["threshold"]) if len(candidates) else 0.5
print("Chosen RF threshold:", CHOSEN_THRESHOLD_RF)

val_pred = (val_proba >= CHOSEN_THRESHOLD_RF).astype(int)
print(classification_report(y_val, val_pred))

# --- TEST (same threshold) ---
test_proba = rf.predict_proba(X_test)[:, 1]
print("RF TEST ROC–AUC:", roc_auc_score(y_test, test_proba))

test_pred = (test_proba >= CHOSEN_THRESHOLD_RF).astype(int)
print(classification_report(y_test, test_pred))



# ========================= Logistic Regression below =========================
# Naive Baseline Model: Predict Majority Class (No Default)

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, classification_report

log_model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", LogisticRegression(
        max_iter=2000,
        random_state=42
    ))
])

log_model.fit(X_train, y_train)

val_proba = log_model.predict_proba(X_val)[:, 1]
print("Logistic VAL ROC-AUC:", roc_auc_score(y_val, val_proba))

val_pred = (val_proba >= 0.5).astype(int)
print(classification_report(y_val, val_pred))

# Balanced Class Model: Adjust for Class Imbalance

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, classification_report

log_model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", LogisticRegression(
        max_iter=2000,
        class_weight="balanced",
        random_state=42
    ))
])

log_model.fit(X_train, y_train)

val_proba = log_model.predict_proba(X_val)[:, 1]
print("Logistic VAL ROC-AUC:", roc_auc_score(y_val, val_proba))

val_pred = (val_proba >= 0.5).astype(int)
print(classification_report(y_val, val_pred))

# Model retraining with adjusted hyperparameters

for C in [1.0, 0.5, 0.2, 0.1, 0.05, 0.01]:
    model = Pipeline(steps=[
        ("prep", preprocess),
        ("clf", LogisticRegression(
            max_iter=5000,
            class_weight="balanced",
            # Regularization process
            C=C,
            solver="liblinear",
            random_state=42
        ))
    ])
    model.fit(X_train, y_train)
    proba = model.predict_proba(X_val)[:, 1]
    auc = roc_auc_score(y_val, proba)
    print(f"C={C:<5}  VAL ROC-AUC={auc:.4f}")

    # Selected logistic regression model with C=0.01

log_model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", LogisticRegression(
        max_iter=5000,
        class_weight="balanced",
        C=0.01,
        solver="liblinear",
        random_state=42
    ))
])

log_model.fit(X_train, y_train)
val_proba = log_model.predict_proba(X_val)[:, 1]
print("Logistic VAL ROC-AUC:", roc_auc_score(y_val, val_proba))

val_pred = (val_proba >= 0.5).astype(int)
print(classification_report(y_val, val_pred))

from sklearn.metrics import precision_recall_curve

val_proba = log_model.predict_proba(X_val)[:, 1]

precision, recall, thresholds = precision_recall_curve(y_val, val_proba)

pr_df = pd.DataFrame({
    "threshold": thresholds,
    "precision": precision[:-1],
    "recall": recall[:-1],
})

# pick recall-prioritised operating point
pr_df[(pr_df["recall"] >= 0.65) & (pr_df["recall"] <= 0.80)] \
    .sort_values("precision", ascending=False) \
    .head(10)

# Model testing Logistic Regression with chosen threshold

CHOSEN_THRESHOLD = 0.74

test_proba = log_model.predict_proba(X_test)[:, 1]

test_pred = (test_proba >= CHOSEN_THRESHOLD).astype(int)

from sklearn.metrics import roc_auc_score, classification_report
print("TEST ROC–AUC:", roc_auc_score(y_test, test_proba))
print(classification_report(y_test, test_pred))
